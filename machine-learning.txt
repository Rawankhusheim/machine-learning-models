Get the Data
Create Isolated Environment

conda create --name learning
conda info --envs
conda activate learning
conda list
conda activate base
conda env remove -n learning
conda install jupyter matplotlib numpy pandas scipy scikit-learn


Get the Data
Download the Data

Write a samll function to:
	Download a single compressed file, housing.tgz which contains comma-separated value (CSV)
	Decompress the file and extract the CSV
It is usefull to have function for these repeatable steps
	if you need to install the data set on multiple machines
	If the data changes regularly


Get the Data
Take a quick Look at the Data Strcture
https://www.youtube.com/watch?v=P4F3PzCMrtk
1. Take a look at the top 5 rows housing.head()
2. Get quick description of the data housing.info()
3. Explore Categorical attributes housing["ocean_proximity"].value_counts()
4. Explore Numrical attributes housing.describe()
	std: measures how dispersed the values are
		it is root of the variance, which is the avarage of the squared deviation from the mean
		when feature has bell-shaped "68, 95, 99.7" rule applies
		mean += std, mean += 2std, mean += 3std
	percentiles: indicates the value below a given percentage of observations
5. Plot histogram for numrical attributes
	y: number of instances
	x: given value range
	Plot histogram for specific attribute housing['median_house_value'].hist(bins=50, figsize=(20,15))
	Plot histogram for each numrical attribute housing.hist(bins=50, figsize=(20,15))
Notes
1. median income attribute is 
	scaled 
		at 10,000 of USD
	capped 
		15 for higher median incomes (actually 15.0001)
		0.49 for lower median incomes (actually 0.49999)
2. housing_median_age, and median_house_value are also capped
	Machine Learning algorithm may learn that prices never go beyon that limit
	If this is problem you have 2 options
		a. Collect proper lables for districts whose labels were capped
		b. Remove those districts from the training, and testing sets
3. attributes have very different scales, we will explore feature scalling later.
4. Many histograms are tail heavy, we will try transforming these attributes later to have more bell-shaped distributions as this shape make it harder for machine learning algorithm to detect patterns.

Hopefully you now have a better understanding of the kind of data you are dealing with
but that enough for now, you need to create a test set, put it aside and never look at it.


Get the Data
Create a Test Set

Data Snooping Bias
- Your brain is an amazing pattern detection system, it is highly prone to overfitting: if you look at the test set
- You may see interesting pattern in the test data that leads you to select specific model
- Then Generalization Error of Test set will be too optimistics

Create a Test Set
	Theoretically quite simple, just pick some instances randomly, typically 20% and set them aside:
	CODING
	Work! But it is not perfect
	Because when run the program again, it will generate a different test set! then you get to see the whole dataset, which is what you want to avoid
	Solutions
	1. Save the test set on the first run then load it in subsequent runs
	2. Set the random number generator's seed np.random.seed(42)
	Work! But it is not perfect
	Because both will break next time you fetch an updated dataset.
	Solutions
	1. Use Instance's identifier to decide whether or not it should go in the test set
	2. Compute a hash of each instance and put that instance in test setif the hash is lower or equal to 20% of the maximum hash value
	CODING

Discover and Visualize the Data to Gain Insights
Visualizing Geographical Data
- Draw Geographical Information
	Create Scatterplot of all districts to visualize the data
	Play with alpha to make it easier to visualize places where is high density
	Let make radius of circle represents the district's population
	Color represent the price ranges from blue (low values) to red (high prices)
	https://www.youtube.com/watch?v=P4F3PzCMrtk
	Findings
	1. Housing Price are very much related to the location and population density
	2. We could use clustering algorithm to detect main clusters and use it as new feature for input, but North prices are not too high
	3. We could use Ocean proximity attribute as well

Discover and Visualize the Data to Gain Insights
Looking for Correlations
	Correlation coefficient ranges from 1 to -1 
		Pearson's r means if x goes up, then y goes up/down
		1 strong positive linear correlation
		-1 strong negative linear correlation
		0 there is no linear correlation

	Scatter Matrix plot numrical attribute against every other numrical attributes
		Define most promising attributes that seem most correlated with median housing value
		Main diagonal will not be useful as it the attribute against itself Pandas draw the histogram of the attribute
		median_income is the most promising attribute to predict median_house_value
		Findings
		1. Correlation is indeed very strong
		2. Upward trend is clear, and points are not too dispersed
		3. Price cap at 500,000$ that appear in histogram
		4. There are another lines on 450,000$, 350,000$, 280,000$

Discover and Visualize the Data to Gain Insights
Experimenting with Attribute Combinations
	You need to combin attributes with eachother and restudy the correlation searching for another relations
	total_rooms/households, total_bedrooms/total_rooms, population/household
	These proposed comibniation could be helpfull Why?
	- lower bedroom/room houses tend to be more expensive.
	- rooms/household is more informative than total_rooms/district
	- larger houses more expensive

These kind of exploration, combiniation and interpretation need to be iterative process


Prepare the Data for Machine Learning Algorithm
- Separate the predictors and the labels as it is not necessary to apply the same transformation to both
housing = strat_train_set.drop('median_house_value', axis=1)
housing_labels = strat_train_set['median_house_value'].copy()


Prepare the Data for Machine Learning Algorithm
Data Cleaning
- Take care of missing values of total_bedrooms as most ML Algorithm cannot work with missing features we have 3 options
	1. Remove Corresponding Districts
	2. Remove whole attributes
	3. Set the values to some value (zero, mean, median, etc)
		- You need to calculate mean on the training set and use it fill up missing values
		- Save mean to use on test set when you evaluate the model, and on live system when new data is came
		- Scikit-Lean provide handy class to fill up missing value with whatever strategy you choose
		- Since median computed only on numrical attribute, we need to drop ocean_proximity as it is categorical
		- SimpleImputer compute median of each attribute and store the result in statistics_
		- It is safer to apply imputer to all numerical attributes
		- call fit method to calculate mean value for all numrical attributes
		- call transform method to apply mean to missing values 
		
		
Prepare the Data for Machine Learning Algorithm
Handling Text and Categorical Attributes
- Most machine learning algorithm prefer to work with number
- Use fit_transform from OrdinalEncoder to convert texts to numbers
- Check categories_ attribute to list all categories
Finding
- Machine Learning will learn that nearby values are more similar than distant values
- Which may be good in some cases like (bad, average, good, excellent)
- But it is not the case with ocean_proximity
		
	