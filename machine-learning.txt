Get the Data
Create Isolated Environment

conda create --name learning
conda info --envs
conda activate learning
conda list
conda activate base
conda env remove -n learning
conda install jupyter matplotlib numpy pandas scipy scikit-learn


Get the Data
Download the Data

Write a samll function to:
	Download a single compressed file, housing.tgz which contains comma-separated value (CSV)
	Decompress the file and extract the CSV
It is usefull to have function for these repeatable steps
	if you need to install the data set on multiple machines
	If the data changes regularly


Get the Data
Take a quick Look at the Data Strcture
https://www.youtube.com/watch?v=P4F3PzCMrtk
1. Take a look at the top 5 rows housing.head()
2. Get quick description of the data housing.info()
3. Explore Categorical attributes housing["ocean_proximity"].value_counts()
4. Explore Numrical attributes housing.describe()
	std: measures how dispersed the values are
		it is root of the variance, which is the avarage of the squared deviation from the mean
		when feature has bell-shaped "68, 95, 99.7" rule applies
		mean += std, mean += 2std, mean += 3std
	percentiles: indicates the value below a given percentage of observations
5. Plot histogram for numrical attributes
	y: number of instances
	x: given value range
	Plot histogram for specific attribute housing['median_house_value'].hist(bins=50, figsize=(20,15))
	Plot histogram for each numrical attribute housing.hist(bins=50, figsize=(20,15))
Notes
1. median income attribute is 
	scaled 
		at 10,000 of USD
	capped 
		15 for higher median incomes (actually 15.0001)
		0.49 for lower median incomes (actually 0.49999)
2. housing_median_age, and median_house_value are also capped
	Machine Learning algorithm may learn that prices never go beyon that limit
	If this is problem you have 2 options
		a. Collect proper lables for districts whose labels were capped
		b. Remove those districts from the training, and testing sets
3. attributes have very different scales, we will explore feature scalling later.
4. Many histograms are tail heavy, we will try transforming these attributes later to have more bell-shaped distributions as this shape make it harder for machine learning algorithm to detect patterns.

Hopefully you now have a better understanding of the kind of data you are dealing with
but that enough for now, you need to create a test set, put it aside and never look at it.


Get the Data
Create a Test Set

Data Snooping Bias
- Your brain is an amazing pattern detection system, it is highly prone to overfitting: if you look at the test set
- You may see interesting pattern in the test data that leads you to select specific model
- Then Generalization Error of Test set will be too optimistics

Create a Test Set
	Theoretically quite simple, just pick some instances randomly, typically 20% and set them aside:
	CODING
	Work! But it is not perfect
	Because when run the program again, it will generate a different test set! then you get to see the whole dataset, which is what you want to avoid
	Solutions
	1. Save the test set on the first run then load it in subsequent runs
	2. Set the random number generator's seed np.random.seed(42)
	Work! But it is not perfect
	Because both will break next time you fetch an updated dataset.
	Solutions
	1. Use Instance's identifier to decide whether or not it should go in the test set
	2. Compute a hash of each instance and put that instance in test setif the hash is lower or equal to 20% of the maximum hash value
	CODING

Discover and Visualize the Data to Gain Insights
Visualizing Geographical Data
- Draw Geographical Information
	Create Scatterplot of all districts to visualize the data
	Play with alpha to make it easier to visualize places where is high density
	Let make radius of circle represents the district's population
	Color represent the price ranges from blue (low values) to red (high prices)
	https://www.youtube.com/watch?v=P4F3PzCMrtk
	Findings
	1. Housing Price are very much related to the location and population density
	2. We could use clustering algorithm to detect main clusters and use it as new feature for input, but North prices are not too high
	3. We could use Ocean proximity attribute as well

Discover and Visualize the Data to Gain Insights
Looking for Correlations
	Correlation coefficient ranges from 1 to -1 
		Pearson's r means if x goes up, then y goes up/down
		1 strong positive linear correlation
		-1 strong negative linear correlation
		0 there is no linear correlation

	Scatter Matrix plot numrical attribute against every other numrical attributes
		Define most promising attributes that seem most correlated with median housing value
		Main diagonal will not be useful as it the attribute against itself Pandas draw the histogram of the attribute
		median_income is the most promising attribute to predict median_house_value
		Findings
		1. Correlation is indeed very strong
		2. Upward trend is clear, and points are not too dispersed
		3. Price cap at 500,000$ that appear in histogram
		4. There are another lines on 450,000$, 350,000$, 280,000$

Discover and Visualize the Data to Gain Insights
Experimenting with Attribute Combinations
	You need to combin attributes with eachother and restudy the correlation searching for another relations
	total_rooms/households, total_bedrooms/total_rooms, population/household
	These proposed comibniation could be helpfull Why?
	- lower bedroom/room houses tend to be more expensive.
	- rooms/household is more informative than total_rooms/district
	- larger houses more expensive

These kind of exploration, combiniation and interpretation need to be iterative process


Prepare the Data for Machine Learning Algorithm
- Separate the predictors and the labels as it is not necessary to apply the same transformation to both
housing = strat_train_set.drop('median_house_value', axis=1)
housing_labels = strat_train_set['median_house_value'].copy()


Prepare the Data for Machine Learning Algorithm
Data Cleaning
- Take care of missing values of total_bedrooms as most ML Algorithm cannot work with missing features we have 3 options
	1. Remove Corresponding Districts
	2. Remove whole attributes
	3. Set the values to some value (zero, mean, median, etc)
		- You need to calculate mean on the training set and use it fill up missing values
		- Save mean to use on test set when you evaluate the model, and on live system when new data is came
		- Scikit-Lean provide handy class to fill up missing value with whatever strategy you choose
		- Since median computed only on numrical attribute, we need to drop ocean_proximity as it is categorical
		- SimpleImputer compute median of each attribute and store the result in statistics_
		- It is safer to apply imputer to all numerical attributes
		- call fit method to calculate mean value for all numrical attributes
		- call transform method to apply mean to missing values 
		
		
Prepare the Data for Machine Learning Algorithm
Handling Text and Categorical Attributes
- Most machine learning algorithm prefer to work with number
- Use fit_transform from OrdinalEncoder to convert texts to numbers
- Check categories_ attribute to list all categories
Finding
- Machine Learning will learn that nearby values are more similar than distant values
- Which may be good in some cases like (bad, average, good, excellent)
- But it is not the case with ocean_proximity

One-Hot Encoding
Create vector with length equal to number of category
Set 1 (hot) corresponding to category index
Set 0 (cold) elsewhere
Scikit-Learn provide OneHotEncoder class to convert categorical values into one-hot vectors
Using fit_transform from OneHotEncoder return SciPy sparse matrix
https://developpaper.com/instructions-for-using-python-scipy-sparse-matrix/
Much efficient for memory usage 
It store location of non-zero element, Instead of store wastefull zeros


Prepare the Data for Machine Learning Algorithm
Custom Transformers

You need to write your transforms for tasks such as custom cleanup operations or combining specific attributes.
To make custom transformers work seamlessly with Scikit-Learn functionalities (such as pipelines) you need to implement fit() (returning self), transform(), and fit_transform().
You can get fit_transform() free by add TransformerMixin as base class
You can achive automatic hyperparameter tuning by add BaseEstimator as another base class which provide get_params() and set_params()	
CODING
We create one hyperparameter, add_bedrooms_per_room set to True by default
It is good to provide sensible defaults
Hypterparameter are parameters that are part of Algorithm not part of the Learning 
We can add hyperparameter that gate any data preparation step that you are not 100% sure about.
More automation more likely you find great combination and Saving you a lot of time


Prepare the Data for Machine Learning Algorithm
Feature Scaling
Mostly Machine Learning algorithms donâ€™t perform well with numerical input attributes that have very different scales.
Feature Scaling need to be done on input to fix such these difrences total_rooms range 6 to 39,320, median_income range 0 to 15
Generally scaling not required for output or target values
We have to common ways
- min-max scaling
- Standardization

Min-Max Scaling (Normalization)
Simply values are shifted and rescaled so that they end up ranging from 0 to 1.
We do that by (x - min(X)) / max(X) - min(X)
Scikit-Learn provide MinMaxScaler transformer which has feature_range hyperparameter that allow you to change the range of values

Standardization
We do that by (x - mean(X)) / std(X)
- Values always have zero mean
- Values resulting distribution has unit variance (std = 1)
- Much less affected by outliers
- Does not bound values to specific range (sometimes this is problem eg neural networks often expect input range 0 to 1)
- Scikit-Learn provide StandardScaler transformer


Prepare the Data for Machine Learning Algorithm
Transformation Pipelines
As we have seen there are many data transformation steps that need to be executed in the right order (Imputer, Attributes Adder, Std Scaler)
Pipline constructor takes list of tuple (name, transformer) defining a sequence of steps
All steps by the last must be transformers (they must have fit_transform() method)
When fit() method called all transformers fit_transform() methods are called sequentially passing output of each call as parameter to the next call
Final estimator just calls the fit() method.
Pipeline exposes same method as final estimator

Scikit-Learn provide ColumnTransformer to handle numerical columns separately from categorical columns
- Constructor require list of tuples (name, transformer or drop or passthrough, list of columns (names or indices)) that the transformer should apply to
- Transformer return same number of rows, but may different number of columns
- When such mix of sparse and dense matrices are exists ColumnTransform estimates the density of the final matrix (ratio of non zero cells)
- Return sparse matrix if sparse_threshold=03 is exceeded


Select and Train a Model
We framed the problem
Got the data and explored it
Sampled a training and test sets.
Wrote transformation pipelines to clean up and prepare data

And we are ready to select and traing model



Select and Train a Model
Training and Evaluating on the Training Set

Thanks to all previous steps that make things much simpler than we might think
Train first Linear Regression model is just like 3 lines of code

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

Done! Let's try it out


Select and Train a Model
Training and Evaluating on the Training Set
Works, but predictions are not exactly accurate
Let's measure RMSE on the whole training set
Scikit-Learn provide mean_squared_error